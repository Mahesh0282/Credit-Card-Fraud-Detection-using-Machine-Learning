# -*- coding: utf-8 -*-
"""credit_card_fraud_detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d_YLZR24_rCUaybpG9HgZqbZrkY74zc1
"""

!pip install deap

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('transactions.csv')

data.shape

data.head()

data = data.drop(['nameOrig', 'nameDest'], axis=1)

data.head()

data = data.dropna()

X = data.drop('isFraud', axis=1)
y = data['isFraud']

categorical_features = ['type']
numerical_features = ['amount', 'oldbalanceOrg', 'newbalanceOrig', 'oldbalanceDest', 'newbalanceDest']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first'), categorical_features)
    ])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)

smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(preprocessor.fit_transform(X_train), y_train)

X_test_transformed = preprocessor.transform(X_test)

models = {
    'Logistic Regression': LogisticRegression(random_state=42),
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'Random Forest': RandomForestClassifier(random_state=42, n_estimators=50),
    'Gradient Boosting (XGBoost)': XGBClassifier(random_state=42, max_depth=6, eval_metric='logloss'),
    'Naive Bayes': GaussianNB()
}

results = {}
for name, model in models.items():
    # Train the model
    model.fit(X_train_resampled, y_train_resampled)

    # Predict on test set
    y_pred = model.predict(X_test_transformed)
    y_proba = model.predict_proba(X_test_transformed)[:, 1] if hasattr(model, 'predict_proba') else model.decision_function(X_test_transformed)

    # Calculate metrics
    report = classification_report(y_test, y_pred, output_dict=True)
    auc_score = roc_auc_score(y_test, y_proba)
    cm = confusion_matrix(y_test, y_pred)
    acc = accuracy_score(y_test, y_pred)

    results[name] = {
        'Accuracy': acc,
        'Precision (Fraud)': report['1']['precision'],
        'Recall (Fraud)': report['1']['recall'],
        'F1-Score (Fraud)': report['1']['f1-score'],
        'AUC-ROC': auc_score,
        'Confusion Matrix': cm
    }

from sklearn.metrics import classification_report

print("\n Classification Report for Each Model:\n")

for name, model in models.items():
    print(f"=== {name} ===")

    # Predict on test set
    y_pred = model.predict(X_test_transformed)

    # Print full classification report
    print(classification_report(y_test, y_pred, digits=4))
    print("-" * 60)

for name, metrics in results.items():
    print(f"\nModel: {name}")
    print(f"Accuracy: {metrics['Accuracy']:.4f}")
    print(f"Precision (Fraud): {metrics['Precision (Fraud)']:.4f}")
    print(f"Recall (Fraud): {metrics['Recall (Fraud)']:.4f}")
    print(f"F1-Score (Fraud): {metrics['F1-Score (Fraud)']:.4f}")
    print(f"AUC-ROC: {metrics['AUC-ROC']:.4f}")
    print(f"Confusion Matrix:\n{metrics['Confusion Matrix']}")

from sklearn.metrics import classification_report
from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# ------------------ PART 1: CLASSIFICATION REPORT ANALYSIS ------------------

model_scores = {}

print("\nüìã Classification Report Evaluation:\n")

for name, model in models.items():
    y_pred = model.predict(X_test_transformed)
    report = classification_report(y_test, y_pred, output_dict=True)

    fraud_metrics = report['1']  # class '1' = fraud
    f1 = fraud_metrics['f1-score']
    recall = fraud_metrics['recall']
    precision = fraud_metrics['precision']

    model_scores[name] = {
        'F1-Score (Fraud)': f1,
        'Recall (Fraud)': recall,
        'Precision (Fraud)': precision
    }

    print(f"=== {name} ===")
    print(f"Precision (Fraud): {precision:.4f}")
    print(f"Recall (Fraud):    {recall:.4f}")
    print(f"F1-Score (Fraud):  {f1:.4f}")
    print("-" * 40)

# Select best model based on F1-Score
best_model_f1 = max(model_scores.items(), key=lambda x: x[1]['F1-Score (Fraud)'])
print(f"\n‚úÖ Best Model Based on F1-Score (Fraud): {best_model_f1[0]}")
print(f"F1-Score (Fraud): {best_model_f1[1]['F1-Score (Fraud)']:.4f}")

# ------------------ PART 2: OVERALL BEST MODEL USING MULTIPLE METRICS ------------------

# Important metrics to evaluate from results
important_metrics = [
    'Accuracy',
    'Precision (Fraud)',
    'Recall (Fraud)',
    'F1-Score (Fraud)',
    'AUC-ROC'
]

# Create a DataFrame from results dictionary
results_df = pd.DataFrame(results).T
metrics_df = results_df[important_metrics]

# Normalize all metrics using MinMaxScaler (to range [0,1])
scaler = MinMaxScaler()
normalized_metrics = pd.DataFrame(
    scaler.fit_transform(metrics_df),
    columns=important_metrics,
    index=metrics_df.index
)

# Compute average score for each model
normalized_metrics['Overall Score'] = normalized_metrics.mean(axis=1)

# Get best model based on overall score
best_overall_model = normalized_metrics['Overall Score'].idxmax()
best_overall_score = normalized_metrics.loc[best_overall_model, 'Overall Score']

# ------------------ PRINT FINAL RESULTS ------------------

print("\nüìä Combined Metric Ranking (Normalized Scores):")
print(normalized_metrics.sort_values('Overall Score', ascending=False))

print(f"\nüèÜ Best Overall Model Based on All Metrics: {best_overall_model}")
print(f"Overall Combined Score: {best_overall_score:.4f}")

from sklearn.metrics import classification_report

model_scores = {}

print("\nüìã Classification Report Evaluation:\n")

for name, model in models.items():
    y_pred = model.predict(X_test_transformed)
    report = classification_report(y_test, y_pred, output_dict=True)

    fraud_metrics = report['1']  # class '1' = fraud
    f1 = fraud_metrics['f1-score']
    recall = fraud_metrics['recall']
    precision = fraud_metrics['precision']

    model_scores[name] = {
        'F1-Score (Fraud)': f1,
        'Recall (Fraud)': recall,
        'Precision (Fraud)': precision
    }

    print(f"=== {name} ===")
    print(f"Precision (Fraud): {precision:.4f}")
    print(f"Recall (Fraud):    {recall:.4f}")
    print(f"F1-Score (Fraud):  {f1:.4f}")
    print("-" * 40)

# Select best model based on F1-Score (Fraud)
best_model = max(model_scores.items(), key=lambda x: x[1]['F1-Score (Fraud)'])

print(f"\n‚úÖ Best Model Based on F1-Score (Fraud): {best_model[0]}")
print(f"F1-Score (Fraud): {best_model[1]['F1-Score (Fraud)']:.4f}")

best_model = max(results.items(), key=lambda x: x[1]['F1-Score (Fraud)'])
print(f"\nBest Model: {best_model[0]}")
print(f"Best F1-Score (Fraud): {best_model[1]['F1-Score (Fraud)']:.4f}")

best_model = max(results.items(), key=lambda x: x[1]['Accuracy'])
print(f"\nBest Model: {best_model[0]}")
print(f"Best Accuracy: {best_model[1]['Accuracy']:.4f}")